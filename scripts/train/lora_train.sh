set -x

python3 -m verl.trainer.main_ppo \ 
    algorithm.adv_estimator=grpo \ # 指定使用的优势估计器（Advantage Estimator）是 GRPO（Group Relative Policy Optimization）。这意味着它会利用同一 Prompt 的多个 Rollout 之间的相对奖励来进行策略更新
    trainer.val_before_train=False \ # 训练前不进行预先验证
    data.train_files=$HOME/data/gsm8k/train.parquet \
    data.val_files=$HOME/data/gsm8k/test.parquet \
    data.train_batch_size=16 \ # Rollout 阶段数据收集的批次大小
    data.max_prompt_length=512 \
    data.max_response_length=1024 \
    data.filter_overlong_prompts=True \ # 过滤掉长度超过 max_prompt_length 的 Prompt。
    data.truncation='error' \ # 如果数据处理中发生截断，则抛出错误，以保证数据完整性。
    data.shuffle=False \ # 在训练期间不打乱数据顺序
    actor_rollout_ref.model.path=Qwen/Qwen2.5-3B-Instruct \ # 指定作为 Actor/策略模型的基座模型为 Qwen2.5-3B-Instruct
    actor_rollout_ref.model.lora_rank=64 \
    actor_rollout_ref.model.lora_alpha=32 \
    actor_rollout_ref.actor.optim.lr=3e-6 \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.actor.ppo_mini_batch_size=16 \ # 将收集到的数据进一步切分成的迷你批次 (Mini Batch) 大小
    actor_rollout_ref.actor.ppo_micro_batch_size_per_gpu=40 \ # 在 Rollout 阶段计算 log-prob 时，设置每个 GPU 的微批次大小为 40
    actor_rollout_ref.actor.use_kl_loss=True \ # 在 Actor 更新的损失函数中使用 KL 散度损失，通常用于限制新旧策略的差异
    actor_rollout_ref.actor.kl_loss_coef=0.001 \ # 设置 KL 损失的系数，用于平衡策略优化和策略约束。
    actor_rollout_ref.actor.kl_loss_type=low_var_kl \ # 明确使用的 KL 损失类型，表明框架针对 KL 散度进行了低方差优化。
    actor_rollout_ref.actor.entropy_coeff=0 \ # 熵损失系数为 0，即禁用熵正则化。熵损失通常用于鼓励模型探索，这里选择不使用。
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.fsdp_config.param_offload=False \ # 在 Actor 训练阶段，选择不使用 FSDP 的参数/优化器卸载功能。这会占用更多 GPU 显存，但能加快训练速度
    actor_rollout_ref.actor.fsdp_config.optimizer_offload=False \
    actor_rollout_ref.rollout.log_prob_micro_batch_size_per_gpu=40 \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \ # 在 Rollout 阶段，将模型切分到 2 个 GPU 上进行张量并行计算
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.6 \ # 限制 vLLM 在 Rollout 时只占用 GPU 显存的 60%
    actor_rollout_ref.rollout.n=5 \ # 每个 Prompt 将生成 5 个不同的 Response（轨迹）
    actor_rollout_ref.rollout.load_format=safetensors \
    actor_rollout_ref.rollout.layered_summon=True \
    actor_rollout_ref.ref.log_prob_micro_batch_size_per_gpu=40 \
    actor_rollout_ref.ref.fsdp_config.param_offload=True \
    algorithm.use_kl_in_reward=False \ # 在奖励计算中不使用 KL 散度惩罚项
    trainer.critic_warmup=0 \
    trainer.logger='["console","wandb"]' \
    trainer.project_name='verl_grpo_example_gsm8k' \
    trainer.experiment_name='qwen2.5_3b_grpo_lora' \
    trainer.n_gpus_per_node=2 \ # 每个节点使用 2 个 GPU
    trainer.nnodes=1 \ # 只有一个节点
    trainer.save_freq=20 \ # 每20步保存一次模型检查点
    trainer.test_freq=5 \ # 每5步在验证集上进行评估
    trainer.total_epochs=15 $@

    # actor_rollout_ref.actor.ppo_mini_batch_size=256 \
    # data.train_batch_size=1024 \
    # trainer.n_gpus_per_node=8 \
    # actor_rollout_ref.model.use_shm=True \